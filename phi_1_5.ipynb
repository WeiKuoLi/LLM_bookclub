{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUA8bfD/fZ06FQN5DhmEZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4b5a097f2af4a6f8316a8f9b2b23479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_111ed3b6213a4ef5bd5d3809884d03b1",
              "IPY_MODEL_45b688f8fb4049b2bcb45cea25057114",
              "IPY_MODEL_2e783d6ef13d44f9801e72347d1463d3"
            ],
            "layout": "IPY_MODEL_1ead5e16eba048608ce8152f9e658153"
          }
        },
        "111ed3b6213a4ef5bd5d3809884d03b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569be345c34c4f17b806f2fd0a50591d",
            "placeholder": "​",
            "style": "IPY_MODEL_44e722785e3740b183874dca615b43eb",
            "value": "Tokenizing data: 100%"
          }
        },
        "45b688f8fb4049b2bcb45cea25057114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161c5a57f8894b5e9d8cd9a1238e55e4",
            "max": 1412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b67ca302c3ed4d2b824827b21ac7212b",
            "value": 1412
          }
        },
        "2e783d6ef13d44f9801e72347d1463d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de834ea7f47e4a609cef02a6b6bd3247",
            "placeholder": "​",
            "style": "IPY_MODEL_e505a7b8648a46afba90610901e4c300",
            "value": " 1412/1412 [00:01&lt;00:00, 1368.66 examples/s]"
          }
        },
        "1ead5e16eba048608ce8152f9e658153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569be345c34c4f17b806f2fd0a50591d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e722785e3740b183874dca615b43eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "161c5a57f8894b5e9d8cd9a1238e55e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b67ca302c3ed4d2b824827b21ac7212b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de834ea7f47e4a609cef02a6b6bd3247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e505a7b8648a46afba90610901e4c300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeiKuoLi/LLM_bookclub/blob/main/phi_1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate transformers einops datasets peft bitsandbytes --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt8ZB5nEg-w0",
        "outputId": "af199e33-564f-4759-8168-286c149325ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGk6cQ9SgOyH"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "oOTOkCTihDbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dee297-42b9-4887-ac9f-545796493f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-1_5\",\n",
        "    device_map={\"\":0},\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "qeL4qkvThnHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model\n",
        "device = next(model.parameters()).device\n",
        "device.type"
      ],
      "metadata": {
        "id": "AOyr9exciL9P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "95e98db1-5cfd-46d5-96ef-00eb88ccea32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=20,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET8DXw4qrJyY",
        "outputId": "c3ccc80e-2999-42bd-d2c1-5f502d33852b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 17,694,720 || all params: 1,435,965,440 || trainable%: 1.2322524976645677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sample):\n",
        "    tokenized_text =  tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "    return tokenized_text\n",
        "\n",
        "data = load_dataset(\"BI55/MedText\", \"default\", split=\"train\")\n",
        "\n",
        "data_df = data.to_pandas()\n",
        "data_df[\"text\"] = data_df[[\"Prompt\", \"Completion\"]].apply(lambda x: \"Prompt: \" + x[\"Prompt\"] + \" Completion: \" + x[\"Completion\"], axis=1)\n"
      ],
      "metadata": {
        "id": "qnIJk8vBrPYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head(), len(data_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbA42IOht1dG",
        "outputId": "f0830519-4048-46b6-fbc9-ffcdebc86f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                              Prompt  \\\n",
              " 0  A 50-year-old male presents with a history of ...   \n",
              " 1  A 7-year-old boy presents with a fever, headac...   \n",
              " 2  A 35-year-old woman presents with a persistent...   \n",
              " 3  A 50-year-old male presents with severe abdomi...   \n",
              " 4  A newborn baby presents with eye redness and a...   \n",
              " \n",
              "                                           Completion  \\\n",
              " 0  This patient's history of recurrent kidney sto...   \n",
              " 1  This child's symptoms of a red, bulging tympan...   \n",
              " 2  While the symptoms might initially suggest ast...   \n",
              " 3  The patient's symptoms suggest an incarcerated...   \n",
              " 4  The infant's symptoms suggest neonatal conjunc...   \n",
              " \n",
              "                                                 text  \n",
              " 0  Prompt: A 50-year-old male presents with a his...  \n",
              " 1  Prompt: A 7-year-old boy presents with a fever...  \n",
              " 2  Prompt: A 35-year-old woman presents with a pe...  \n",
              " 3  Prompt: A 50-year-old male presents with sever...  \n",
              " 4  Prompt: A newborn baby presents with eye redne...  ,\n",
              " 1412)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset.from_pandas(data_df)\n",
        "\n",
        "tokenized_data = data.map(tokenize, batched=True, desc=\"Tokenizing data\", remove_columns=data.column_names)\n",
        "\n",
        "tokenized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "d4b5a097f2af4a6f8316a8f9b2b23479",
            "111ed3b6213a4ef5bd5d3809884d03b1",
            "45b688f8fb4049b2bcb45cea25057114",
            "2e783d6ef13d44f9801e72347d1463d3",
            "1ead5e16eba048608ce8152f9e658153",
            "569be345c34c4f17b806f2fd0a50591d",
            "44e722785e3740b183874dca615b43eb",
            "161c5a57f8894b5e9d8cd9a1238e55e4",
            "b67ca302c3ed4d2b824827b21ac7212b",
            "de834ea7f47e4a609cef02a6b6bd3247",
            "e505a7b8648a46afba90610901e4c300"
          ]
        },
        "id": "rijhWIQTrfxR",
        "outputId": "b9474112-0ba4-465e-b4f6-32caa4c19392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing data:   0%|          | 0/1412 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4b5a097f2af4a6f8316a8f9b2b23479"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 1412\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"phi-1_5-finetuned-med-text-high\",\n",
        "        per_device_train_batch_size=7,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=50,\n",
        "        max_steps=1000,\n",
        "        num_train_epochs=1\n",
        "    )"
      ],
      "metadata": {
        "id": "ZPTuaR0trvIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data,\n",
        "    args=training_arguments,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "eTYU0XMSsPJG",
        "outputId": "61384278-ca43-4d12-9b7c-7c1285fa92ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 18:15, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.578400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.444600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.404700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.380600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.250600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.254000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.241700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.252000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.091700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.072000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.090900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.080700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.967200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.921500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.948100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.945600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.863400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.871800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.848700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.862700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=1.1185302772521972, metrics={'train_runtime': 1098.4158, 'train_samples_per_second': 6.373, 'train_steps_per_second': 0.91, 'total_flos': 1.301133421215744e+16, 'train_loss': 1.1185302772521972, 'epoch': 4.95})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"phi-1_5-finetuned-med-text-high\")"
      ],
      "metadata": {
        "id": "13T6yo_7sSQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "H9KxSDM9w76Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32)\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(model, \"phi-1_5-finetuned-med-text-high\", from_transformers=True)\n",
        "\n",
        "model = peft_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "aVsa4qN2scV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvhP63kVxJUG",
        "outputId": "23e715bd-1842-4681-9f7d-865aa1c4d63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2048)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x PhiDecoderLayer(\n",
              "        (self_attn): PhiAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_pretrained(\"phi-1_5-finetuned-med-text-high\")\n",
        "\n",
        "# model.push_to_hub(\"llm-exp/phi-1_5-finetuned-med-text\")"
      ],
      "metadata": {
        "id": "FQDlQOZ3scts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Replace 'path_to_your_model_directory' with the actual path to your model directory\n",
        "model_path = \"./phi-1_5-finetuned-med-text-high\"\n",
        "\n",
        "# Load the model from the local directory\n",
        "model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "# Now you can use the model as usual\n"
      ],
      "metadata": {
        "id": "tkzylFTLyNOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"phi-1_5-finetuned-med-text\", trust_remote_code=True, torch_dtype=torch.float32)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "\n",
        "inputs = tokenizer('Prompt: I am allergic to peanuts, can i eat cashew? Completion:', return_tensors=\"pt\", return_attention_mask=False)\n",
        "inputs.to(device)\n",
        "outputs = model.generate(**inputs, max_length=512)\n",
        "\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pswtGXyOsc5s",
        "outputId": "5efeb8bc-a78a-4d16-f107-200f97cf9a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: I am allergic to peanuts, can i eat cashew? Completion: Yes, you can eat cashews if you are not allergic to other nuts. What's important is to avoid peanuts if you are allergic. If you are not allergic, cashews are a safe option. Just be aware that cashews are still a nut and may contain traces of peanuts, so it's possible to have a reaction even if you are not allergic to peanuts. If you have a severe allergy, it's best to avoid all nuts. If you are unsure about the allergen content of a food, it's best to avoid it. If you do eat a food and have a reaction, carry your epinephrine auto-injector and seek medical attention immediately. If you have a known allergy, it's also important to carry an epinephrine auto-injector and to inform others about your allergy. If you have a severe allergy, you should carry an epinephrine auto-injector at all times. If you have a mild allergic reaction, you may be able to treat it at home with over-the-counter antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with antihistamines. If you have a severe allergic reaction, seek immediate medical attention. If you have a mild allergic reaction, you may be able to treat it at home with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbsm8xbBzgFG",
        "outputId": "0c5ad3ed-7f5e-49fe-c395-9212856b585f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli upload phi-1_5-finetuned-med-text-high"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPPI-UUfAgBK",
        "outputId": "2605b407-7d86-466a-f3d8-3b4b92b03f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster uploads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "\n",
            "adapter_model.safetensors:   0% 0.00/70.8M [00:00<?, ?B/s]\n",
            "\n",
            "\n",
            "rng_state.pth:   0% 0.00/14.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 27 LFS files:   0% 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   0% 16.4k/70.8M [00:00<17:26, 67.6kB/s]\n",
            "adapter_model.safetensors:   0% 16.4k/70.8M [00:00<18:02, 65.4kB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 16.4k/142M [00:00<35:26, 66.7kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 53.3kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 4.05kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:   3% 4.72M/142M [00:00<00:08, 17.0MB/s]\u001b[A\u001b[A\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 2.74kB/s]\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 34.0kB/s]\n",
            "adapter_model.safetensors:  10% 7.11M/70.8M [00:00<00:03, 20.2MB/s]\n",
            "adapter_model.safetensors:   6% 4.47M/70.8M [00:00<00:05, 12.3MB/s]\u001b[A\n",
            "adapter_model.safetensors:  14% 9.90M/70.8M [00:00<00:02, 23.3MB/s]\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  13% 9.42M/70.8M [00:00<00:03, 17.0MB/s]\n",
            "\n",
            "optimizer.pt:  10% 13.8M/142M [00:00<00:05, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "training_args.bin:   0% 0.00/4.98k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  18% 12.5M/70.8M [00:00<00:03, 18.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 36.8kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   2% 1.49M/70.8M [00:00<00:06, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  23% 16.0M/70.8M [00:00<00:03, 17.0MB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  18% 25.2M/142M [00:01<00:03, 30.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  33% 23.1M/70.8M [00:01<00:01, 25.0MB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:   0% 328k/142M [00:00<00:44, 3.16MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   5% 3.47M/70.8M [00:00<00:07, 8.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:   5% 7.63M/142M [00:00<00:03, 40.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  39% 27.9M/70.8M [00:01<00:01, 24.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   8% 5.49M/70.8M [00:00<00:05, 11.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  31% 21.7M/70.8M [00:01<00:02, 18.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  10% 7.24M/70.8M [00:00<00:05, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  35% 24.6M/70.8M [00:01<00:02, 19.9MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  23% 32.0M/142M [00:01<00:04, 23.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  11% 16.0M/142M [00:00<00:03, 32.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  30% 42.5M/142M [00:01<00:02, 34.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  16% 11.1M/70.8M [00:00<00:03, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  45% 32.0M/70.8M [00:01<00:02, 17.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  20% 14.0M/70.8M [00:00<00:03, 18.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  58% 41.0M/70.8M [00:01<00:01, 27.2MB/s]\n",
            "\n",
            "optimizer.pt:  34% 48.0M/142M [00:01<00:03, 29.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  20% 28.7M/142M [00:00<00:03, 34.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  68% 47.9M/70.8M [00:02<00:00, 31.5MB/s]\n",
            "\n",
            "optimizer.pt:  37% 52.0M/142M [00:02<00:02, 30.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  39% 55.8M/142M [00:02<00:02, 30.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  74% 52.3M/70.8M [00:02<00:00, 26.7MB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:  23% 32.3M/142M [00:01<00:05, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  42% 59.6M/142M [00:02<00:02, 29.3MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  46% 32.8M/70.8M [00:02<00:03, 11.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  79% 55.9M/70.8M [00:02<00:00, 27.1MB/s]\n",
            "\n",
            "optimizer.pt:  44% 62.9M/142M [00:02<00:02, 28.2MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  51% 36.0M/70.8M [00:02<00:02, 14.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  31% 22.1M/70.8M [00:01<00:03, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  30% 42.8M/142M [00:01<00:03, 31.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  54% 38.3M/70.8M [00:02<00:02, 15.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  84% 59.3M/70.8M [00:02<00:00, 23.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  39% 27.8M/70.8M [00:01<00:02, 18.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  47% 66.0M/142M [00:02<00:03, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  34% 48.0M/142M [00:01<00:03, 25.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  52% 73.7M/142M [00:02<00:02, 29.2MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  60% 42.5M/70.8M [00:02<00:02, 13.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  39% 55.4M/142M [00:01<00:02, 32.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  56% 79.0M/142M [00:02<00:01, 32.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  43% 60.7M/142M [00:01<00:02, 34.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  90% 64.0M/70.8M [00:03<00:00, 17.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  96% 68.1M/70.8M [00:03<00:00, 20.5MB/s]\n",
            "\n",
            "optimizer.pt:  59% 83.0M/142M [00:03<00:02, 28.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  54% 37.9M/70.8M [00:02<00:01, 19.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  62% 87.9M/142M [00:03<00:01, 31.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  46% 64.8M/142M [00:02<00:02, 26.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  68% 48.0M/70.8M [00:03<00:01, 12.5MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  65% 91.5M/142M [00:03<00:01, 32.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  60% 42.2M/70.8M [00:02<00:01, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  51% 71.9M/142M [00:02<00:02, 34.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  81% 57.3M/70.8M [00:03<00:00, 24.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  55% 77.6M/142M [00:02<00:01, 36.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  67% 95.2M/142M [00:03<00:01, 28.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  64% 45.0M/70.8M [00:02<00:01, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  88% 62.3M/70.8M [00:03<00:00, 27.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors: 100% 70.8M/70.8M [00:03<00:00, 18.7MB/s]\n",
            "\n",
            "adapter_model.safetensors:  93% 66.0M/70.8M [00:03<00:00, 24.1MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  69% 98.4M/142M [00:03<00:02, 18.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  58% 82.1M/142M [00:02<00:02, 23.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  70% 49.7M/70.8M [00:03<00:01, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  62% 87.7M/142M [00:02<00:01, 28.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  75% 106M/142M [00:04<00:01, 27.4MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  77% 54.5M/70.8M [00:03<00:00, 19.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  65% 91.8M/142M [00:03<00:01, 29.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth:   0% 0.00/14.2k [00:00<?, ?B/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  84% 59.4M/70.8M [00:03<00:00, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 94.4kB/s]\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  68% 96.0M/142M [00:03<00:02, 22.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors: 100% 70.8M/70.8M [00:04<00:00, 15.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt:   0% 0.00/1.06k [00:00<?, ?B/s]\n",
            "\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 8.23kB/s]\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt: 100% 142M/142M [00:05<00:00, 28.2MB/s]\n",
            "\n",
            "adapter_model.safetensors:   0% 0.00/70.8M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 27 LFS files:   7% 2/27 [00:05<00:55,  2.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 33.3kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  90% 64.0M/70.8M [00:04<00:00, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:   6% 4.15M/70.8M [00:00<00:01, 33.8MB/s]\u001b[A\n",
            "adapter_model.safetensors:  12% 8.83M/70.8M [00:00<00:01, 39.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors: 100% 70.8M/70.8M [00:04<00:00, 15.5MB/s]\n",
            "\n",
            "adapter_model.safetensors:  18% 12.9M/70.8M [00:00<00:01, 36.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:   0% 0.00/142M [00:00<?, ?B/s]\n",
            "\n",
            "optimizer.pt:   5% 7.26M/142M [00:00<00:02, 65.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Upload 27 LFS files:  26% 7/27 [00:05<00:08,  2.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 89.3kB/s]\n",
            "optimizer.pt: 100% 142M/142M [00:04<00:00, 30.9MB/s]\n",
            "\n",
            "adapter_model.safetensors:  23% 16.6M/70.8M [00:00<00:03, 17.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 27 LFS files:  30% 8/27 [00:05<00:07,  2.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  16% 22.8M/142M [00:00<00:02, 40.9MB/s]\n",
            "adapter_model.safetensors:  38% 27.1M/70.8M [00:00<00:01, 33.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 2.83kB/s]\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 30.3kB/s]\n",
            "optimizer.pt:  20% 28.4M/142M [00:00<00:02, 44.0MB/s]\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   6% 4.18M/70.8M [00:00<00:03, 18.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  46% 32.2M/70.8M [00:01<00:01, 27.6MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 0.00/142M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth:   0% 0.00/14.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  61% 43.1M/70.8M [00:01<00:00, 41.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 91.9kB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:   3% 4.15M/142M [00:00<00:05, 24.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  27% 38.1M/142M [00:01<00:03, 29.7MB/s]\n",
            "\n",
            "optimizer.pt:   6% 8.42M/142M [00:00<00:04, 27.1MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  69% 49.0M/70.8M [00:01<00:00, 35.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt:   0% 0.00/1.06k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:   9% 12.2M/142M [00:00<00:04, 30.9MB/s]\u001b[A\u001b[A\n",
            "optimizer.pt:  30% 43.2M/142M [00:01<00:03, 25.7MB/s]\n",
            "\n",
            "\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 5.96kB/s]\n",
            "optimizer.pt:  33% 47.3M/142M [00:01<00:03, 28.3MB/s]\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  35% 25.0M/70.8M [00:00<00:01, 30.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  90% 64.0M/70.8M [00:01<00:00, 36.8MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  11% 16.0M/142M [00:00<00:06, 18.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "training_args.bin:   0% 0.00/4.98k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  16% 23.3M/142M [00:00<00:04, 29.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 27.9kB/s]\n",
            "adapter_model.safetensors: 100% 70.8M/70.8M [00:02<00:00, 32.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  40% 57.0M/142M [00:01<00:03, 26.8MB/s]\n",
            "\n",
            "optimizer.pt:  23% 32.0M/142M [00:01<00:03, 29.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  45% 63.5M/142M [00:01<00:02, 33.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Upload 27 LFS files:  44% 12/27 [00:07<00:05,  2.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:   0% 0.00/70.8M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  28% 39.2M/142M [00:01<00:02, 35.4MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  11% 7.95M/70.8M [00:00<00:00, 79.4MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  48% 67.9M/142M [00:02<00:02, 26.6MB/s]\n",
            "adapter_model.safetensors:  22% 15.9M/70.8M [00:00<00:00, 69.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  73% 52.0M/70.8M [00:01<00:00, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  80% 56.7M/70.8M [00:01<00:00, 33.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  50% 71.5M/142M [00:02<00:02, 25.6MB/s]\n",
            "\n",
            "optimizer.pt:  36% 50.3M/142M [00:01<00:03, 30.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "optimizer.pt:  55% 78.3M/142M [00:02<00:01, 32.3MB/s]\n",
            "adapter_model.safetensors:  32% 22.9M/70.8M [00:00<00:01, 32.9MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  38% 54.0M/142M [00:01<00:03, 27.2MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  41% 28.7M/70.8M [00:00<00:01, 38.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  94% 66.7M/70.8M [00:02<00:00, 27.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  58% 82.2M/142M [00:02<00:02, 25.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  62% 87.5M/142M [00:02<00:01, 27.8MB/s]\n",
            "\n",
            "optimizer.pt:  41% 58.5M/142M [00:02<00:03, 21.2MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  48% 33.7M/70.8M [00:00<00:01, 27.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  67% 95.0M/142M [00:03<00:01, 35.1MB/s]\n",
            "\n",
            "optimizer.pt:  43% 61.2M/142M [00:02<00:03, 21.0MB/s]\u001b[A\u001b[A\n",
            "optimizer.pt:  70% 99.1M/142M [00:03<00:01, 31.8MB/s]\n",
            "adapter_model.safetensors:  64% 45.5M/70.8M [00:01<00:00, 37.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  73% 104M/142M [00:03<00:01, 32.1MB/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  16% 23.2M/142M [00:01<00:04, 26.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  78% 111M/142M [00:03<00:00, 38.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  21% 30.4M/142M [00:01<00:03, 34.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  71% 50.4M/70.8M [00:01<00:00, 27.1MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  49% 69.6M/142M [00:02<00:03, 20.8MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  78% 55.5M/70.8M [00:01<00:00, 31.1MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  81% 115M/142M [00:03<00:00, 30.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  24% 34.4M/142M [00:01<00:03, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  56% 79.7M/142M [00:03<00:02, 30.1MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  89% 62.9M/70.8M [00:01<00:00, 36.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  84% 119M/142M [00:03<00:00, 27.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  30% 42.9M/142M [00:01<00:03, 31.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "optimizer.pt:  89% 126M/142M [00:04<00:00, 33.6MB/s]\n",
            "\n",
            "optimizer.pt:  59% 83.6M/142M [00:03<00:02, 23.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  92% 130M/142M [00:04<00:00, 26.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors: 100% 70.8M/70.8M [00:02<00:00, 29.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  38% 54.5M/142M [00:02<00:02, 29.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  68% 96.0M/142M [00:03<00:01, 27.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  45% 63.1M/142M [00:02<00:02, 38.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt: 100% 142M/142M [00:04<00:00, 30.6MB/s]\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 77.1kB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:  79% 112M/142M [00:04<00:00, 30.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  48% 68.0M/142M [00:02<00:02, 27.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt:   0% 0.00/1.06k [00:00<?, ?B/s]\n",
            "\n",
            "optimizer.pt:  86% 122M/142M [00:04<00:00, 42.0MB/s]\u001b[A\u001b[A\n",
            "training_args.bin:   0% 0.00/4.98k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  51% 72.9M/142M [00:02<00:02, 29.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 6.66kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "training_args.bin: 100% 4.98k/4.98k [00:00<00:00, 25.7kB/s]\n",
            "\n",
            "\n",
            "events.out.tfevents.1712384724.6434b9acb7d4.15346.0:   0% 0.00/9.94k [00:00<?, ?B/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  59% 84.4M/142M [00:03<00:01, 29.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "events.out.tfevents.1712384724.6434b9acb7d4.15346.0: 100% 9.94k/9.94k [00:00<00:00, 63.7kB/s]\n",
            "optimizer.pt: 100% 142M/142M [00:04<00:00, 29.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  69% 98.0M/142M [00:03<00:01, 31.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  78% 111M/142M [00:03<00:00, 48.3MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  83% 117M/142M [00:03<00:00, 38.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt: 100% 142M/142M [00:04<00:00, 31.7MB/s]\n",
            "adapter_model.safetensors: 100% 70.8M/70.8M [00:07<00:00, 9.44MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Upload 27 LFS files: 100% 27/27 [00:13<00:00,  1.97it/s]\n",
            "https://huggingface.co/WKLI22/phi-1_5-finetuned-med-text-high/tree/main/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2GXWADgA_aE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}